# 随机场

## 前言
### 二项分布
**二项分布就是重复n次独立的伯努利实验**，并且每次实验中只有两种可能的结果，0和1，并且结果之间相互对立。每次结果相互独立并且与其他各次实验结果无关。则这一系列实验总称为n重伯努利实验，当实验的次数为1时，二次分布服从0-1分布。

若每次试验成功的概率为p，则不成功的概率为(1-p)。n次伯努利实验中发生K次的概率是：
<div align=center>
<a href="http://www.codecogs.com/eqnedit.php?latex=P(X&space;=&space;k)&space;=&space;C_{n}^{k}p^{k}(1-p)^{n-k}&space;\rightarrow&space;B(k;n,p)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?P(X&space;=&space;k)&space;=&space;C_{n}^{k}p^{k}(1-p)^{n-k}&space;\rightarrow&space;B(k;n,p)" title="P(X = k) = C_{n}^{k}p^{k}(1-p)^{n-k} \rightarrow B(k;n,p)" /></a></div>



### 高斯分布
高斯分布就是正态分布。

### 正态分布

正态分布 称之为“高斯分布”或“常态分布”，其正态分布的曲线如下：

<div align=center>
<img src="http://m.qpic.cn/psb?/V14Ifnin2f6pWC/7DawwPPjFujVWMPCCdMB.7vqIXbYxN6F0hyGQA18rtI!/b/dGcBAAAAAAAA&bo=WAKkAQAAAAARB88!&rf=viewer_4" width="500" height="300" alt="马尔科夫链"/></div>

如图所示，此正态分布服从一个期望为μ，方差为σ^2 ==> N(μ,σ^2),当 μ = 0,σ = 1时，此正态分布为标准正态分布。

其概率密度函数为：
<div align=center>
<a href="http://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;\frac{1}{\sqrt{2\pi&space;}&space;\sigma&space;}exp^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x)&space;=&space;\frac{1}{\sqrt{2\pi&space;}&space;\sigma&space;}exp^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}" title="f(x) = \frac{1}{\sqrt{2\pi } \sigma }exp^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}" /></a></div>

在相同条件下，正态分布的信息熵最大，由此可知 正态分布是一种最混乱的系统，故而正态分布与最大熵模型有关。

## 1， 马尔科夫链
马尔科夫链是满足马尔科夫性质的随机过程，其为状态空间中经过从一个状态到另一个状态转换的随机过程，具备**无记忆**的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的时间均与之无关。

马尔科夫链的每一步，根据概率分布可以从一个状态变到另一状态，当然也可以保持当前状态，状态的改变称之为转移，与不同状态改变相关的概率称之为转移概率。

<div align=center>
<img src="http://m.qpic.cn/psb?/V14Ifnin2f6pWC/7mwcBlKBfGqoz0tKfZWkWKlgpKyDfjjbdVnqGc.Xd1Q!/b/dPIAAAAAAAAA&bo=*AAmAQAAAAADB*k!&rf=viewer_4" width="300" height="300" alt="马尔科夫链"/>
</div>

**马尔科夫性质：**
马尔科夫性质是概率论的一个概念，当一个随机过程在给定现在状态及过去所有状态的情况下，其未来状态的条件随机分布仅取决于当前的状态，而与以前的状态无关，也即给定当前状态的情况下，其未来状态与过去状态是独立的。具有马尔科夫性质的过程通常称之为马尔科夫过程。

**m阶马尔科夫链：**
<div align=center>
<a href="http://www.codecogs.com/eqnedit.php?latex=Pr(X_{n}&space;=&space;x_{n}|&space;X_{n-1}&space;=&space;x_{n-1},X_{n-2}&space;=&space;x_{n-2}...X_{2}&space;=&space;x_{2},X_{1}&space;=&space;x_{1})&space;=&space;Pr(X_{n}&space;=&space;x_{n}|&space;X_{n-1}&space;=&space;x_{n-1},X_{n-2}&space;=&space;x_{n-2}...X_{n-m}&space;=&space;x_{n-m})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?Pr(X_{n}&space;=&space;x_{n}|&space;X_{n-1}&space;=&space;x_{n-1},X_{n-2}&space;=&space;x_{n-2}...X_{2}&space;=&space;x_{2},X_{1}&space;=&space;x_{1})&space;=&space;Pr(X_{n}&space;=&space;x_{n}|&space;X_{n-1}&space;=&space;x_{n-1},X_{n-2}&space;=&space;x_{n-2}...X_{n-m}&space;=&space;x_{n-m})" title="Pr(X_{n} = x_{n}| X_{n-1} = x_{n-1},X_{n-2} = x_{n-2}...X_{2} = x_{2},X_{1} = x_{1}) = Pr(X_{n} = x_{n}| X_{n-1} = x_{n-1},X_{n-2} = x_{n-2}...X_{n-m} = x_{n-m})" /></a></div>
*m阶马尔科夫链代表 未来状态取决于其前m个状态*。

**连续时间马尔科夫链：**

###1.2 马尔科夫链蒙特卡洛方法


##2，马尔科夫随机场


## 浅谈自然语言处理--上
## 浅谈自然语言处理--中
## 浅谈自然语言处理--下 


#Tensorflow学习
##Install Tensorflow
Tensorflow的安装有多种形式，本文采用基于 Anaconda 的安装方式，步骤如下：

	1. conda create -n tensorflow python=3.6  --新建一个tensorflow环境
	2. source activate/deactivate tensorflow --激活/停止环境[/users/liyanan/anaconda/envs]
	3. conda search tensorflow  --conda库中搜索tensorflow
	4. conda install tensorflow --conda库中安装tensorflow
	5. conda info -e   --列出当前所有的环境
	 
##Base Tensorflow

Tensorflow的op中兼有GPU和CPU实现[Tensorflow安装时的选择]，那么当算子被指派设备时，GPU有优先权，系统会自动将OP指派到GPU中执行。当然也可以手工指派设备，利用"with tf.device('/cpu:0')"。

1. Tensorflow采用graph(图)来表示计算任务。
2. Tensorflow在Session(绘画)的上下文中执行图。
3. Tensorflow使用tensor表示数据，通过变量(Variable)维护状态。
4. Tensorflow使用feed和fetch方法为操作赋值和拉取数据。

Tensor是一个对象，代表一个op的输出，但是Tensor并不存储op输出的值，而仅是提供了计算这些输出值的一种方式。Tensor数据结构有两个目的：1，作为另一个op的输入，表征图的连接。2，Tensor作为参数传递给Session run，但是run(Tensor...)后的结果并不属于Tensor。
Tensor具有以下几个属性：
	
	1, dtype,代表Tensor包含元素的类型。
	2，name，代表Tensor的字符串名字。
	3，graph,包含此Tensor的Graph。
	4，op,生成此Tensor的op。
	5，consumers,对应op，代表传递此Tensor的op。
	6，value_index，Tensor代表的元素的索引。
	7，get_shape()，返回此Tensor的形状，例如2*3。
	8，eval，在Session中执行此Tensor。
	
Variables也是一种tensor，训练模型时，变量用来存储和更新参数，使用之前需要显示的初始化操作，而在模型训练后必须存放至磁盘中。神经网络中，Variables一般都是模型参数，在每轮训练时保存更新后的参数，并且变量还可以被模型保存和加载。一个Variable代表一个可修改的张量，存在在TensorFlow的用于描述交互性操作的图中。它们可以用于计算输入值，也可以在计算中被修改。


##Tensorflow 数据输入

Tensorflow支持多种输入数据类型，例如csv,txt等。但是Tensorflow推荐数据类型为TFRecords。此种数据类型可以支持QueuRunner和Coordinator进行多线程数据读取，并且可以通过batch size和epoch参数来控制训练时单次batch的大小和对样本文件迭代训练的轮数。


##Examples Tensorflow
本小节讲述常用的机器学习经典事例。
###MNIST  --手写数字识别
####1、 Softmax Regression  Model
多分类问题属于指数分布族，广义线性模型拟合多项分布，由广义线性模型推导出假设函数。[0-1]分布属于伯努利分布，p(y = 1; φ) = φ; p(y = 0; φ) = 1 − φ;伯努利分布转换为指数分布如下：

<a href="http://www.codecogs.com/eqnedit.php?latex=p(y;\o)=\o^{y}*(1-\o)^{1-y}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?p(y;\o)=\o^{y}*(1-\o)^{1-y}" title="p(y;\o)=\o^{y}*(1-\o)^{1-y}" /></a>

Softmax回归模型是逻辑回归在多分类问题上的推广，且是逻辑回归的一般形式。SoftMax回归假设函数输出值为一个向量，且向量中每个值代表属于此类的概率，则其假设函数如下：

<div align=center>
<img src="http://a3.qpic.cn/psb?/V14Ifnin2f6pWC/HY*dcCqTaSB7vMZJ1X8rH1hYeqskgs7KDoKKwfArziQ!/b/dPIAAAAAAAAA&bo=vAYKAgAAAAADB5A!&rf=viewer_4" width="1000" height="180" alt="go struct结构"/>
</div>

从图中可以看出，K个类，每个类都会有一种参数向量，所以K个类训练需要得出K种不同的参数向量。

损失函数如下：

<div align=center>
<img src="http://a3.qpic.cn/psb?/V14Ifnin2f6pWC/Jzo0l1zwMcw0KFRg4U3Rqt1u6667cNrdxptk3yJfZUw!/b/dPIAAAAAAAAA&bo=dAT.AAAAAAADAKs!&rf=viewer_4" width="600" height="80" alt="损失函数"/>
</div>

SoftMax函数中样本属于第j类的概率如下：

<div align=center>
<img src="http://a1.qpic.cn/psb?/V14Ifnin2f6pWC/BjhqA0DU74W8woxRUM2pwF67NA2d7BmZgZTCe.Geazs!/b/dGsBAAAAAAAA&bo=vALOAAAAAAADB1I!&rf=viewer_4" width="300" height="80" alt="样本属于第j个类概率"/>
</div>

训练与逻辑回归类似，也采用梯度下降的方式，但是SoftMax回归模型的参数具有“冗余”性，即最优解不止一个，即从一个参数向量中减去另一个向量，完全不影响其值，这也就导致了假设函数的最优解附近为平的，基本上不引起函数值的变化。根据这个冗余性，可以采用函数衰减的方法解决，代价函数如下：

<div align=center>
<img src="http://a3.qpic.cn/psb?/V14Ifnin2f6pWC/QwHzOnU0MdgZgQKB.6VW5gx2yy1O50P.9*ZgWA9whfU!/b/dPIAAAAAAAAA&bo=8wWAAgAAAAADAFE!&rf=viewer_4" width="1000" height="200" alt="加入衰减因子损失函数"/>
</div>  
其中损失函数的倒数应为参数矩阵，行代表类别，列代表每个类别中特征的权值。

####2、深入mnist数据集

通常的，为了进行高效的矩阵计算，通常情况下我们会使用Numpy这样的矩阵计算库，将一些耗时的操作例如矩阵计算在Python的外部环境中，计算完成后再切入通常的Python环境中，但是这样会产生损耗--数据传输，尤其是在多CPU,GPU的环境下。但是Tensorflow为了避免这种数据传输的损耗，采用了这样的方法：构建一个操作图，然后完全运行此图在Python环境外面。

深入mnist数据集采用卷积神经网络的方法拟合训练数据集，步骤如下:

1. 卷积层和pooling层都为2层[卷积->pooling->卷积->pooling]，在最后一个pooling层结束的时候，其feature map的大小为7*7,共64个feature map。
2. 对所有的feature map作为新的输入，采用全连接的方式，生成新的1024个隐藏层神经单元。
3. 隐藏层神经单元与softmax回归单元相连接，输出10维向量，结束。

图示如下：
<div align=center>
<img src="http://a3.qpic.cn/psb?/V14Ifnin2f6pWC/uZPblmfQcYpVJeC9g3h.aXDQCD9nInBNOa7EYkvKUgM!/b/dPIAAAAAAAAA&bo=bQaAAgAAAAARB9k!&rf=viewer_4" width="800" height="300" alt="cnn_mnist"/>
</div>  

注：1，步骤1中的第二卷积层每个feature map数据计算来源于第一pooling层的所有的32个feature map。2，pooling采用max_pooling的方式。3，第一卷积层的feature map大小为28*28，是因为卷积时采用了padding="SAME"的方法。

####3、 tensorflow通用运作方式



#问题
1. softmax为何将交叉熵作为损失函数?  首先在y取值为一个向量时，那么交叉熵即为上文中的损失函数，交叉熵作为损失函数，依然可以采用逻辑回归中的定义解释：softmax回归依然将损失函数定义为训练样本Y关于X的分布，依然用于描述训练样本的分布，而对此分布采用极大似然函数 即可。
#文献
[1. 浅谈自然语言处理基础（上)](https://www.jianshu.com/p/c123c7534500?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)



  

